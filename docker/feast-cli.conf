onfhir-feast {
  # Configuration for Spark
  spark {
    app.name = "onFHIR Feature Store"
    master = "local[8]"
  }
  # Configurations for CLI interface
  cli {
    # Path to the folder including the definitions (Population, FeatureSet) to process for each category
    definition-folder = "./conf/definitions"
  }
  # Configurations related with supported data interfaces and data sources
  etl {
    # Settings for offline-repository where the extracted population, featuregroup and datasets are stored
    offline-repository {
      # Type of file system to store
      # 'fs' : For normal file system
      # TODO support others e.g. hdfs, s3, etc
      type = "fs"
      # Absolute path to the root directory where all extracted data and temp data will be stored
      rootDir = "./conf/output-data"
    }

    # Data interfaces supported in onfhir-feast (key-value pair where key as the name of interface, value as the configuration)
    data-interfaces {
      # Configuration for FHIR interface
      fhir {
        implementation = "io.onfhir.feast.etl.fhir.FhirDataInterfaceProvider"
        # Supported versions within the onfhir-feast platform
        versions = ["R5"]
        # Further configuration for population extraction execution fo this interface
        population {
          # Execution mode
          # 'phased' : Execution will be divided into phases (evaluation of entry, exit, eligibility, etc) where intermediate results are stored in temp directory
          # 'single-batch': Execution is done as a single Spark Batch job
          execution-mode = "phased"
        }
        # Further configuration for FeatureGroup extraction execution fo this interface
        featureGroup {
          # Execution mode
          # 'online' : Skip synchronization and everytime load and extract entries from the data source
          # 'phased' : Synchronization step is performed in phased mode and results are stored in the dedicated feature group directory and then loaded from there when it is referenced
          execution-mode = "phased"
        }
      }
    }

    # Provides the data source settings for onfhir-feast
    data-sources {
      # Example FHIR API data source configuration (key-value pair where key as the name of data source)
      myFhirServer {
        id = "myFhirServer"
        # Data access interface (one of data-interfaces.*)
        interface = "fhir"
        # Type of data source. Only the followings are supported currently.
        #   - 'fhir-api': Data will be loaded from a target FHIR API as FHIR resources.
        #   - 'fhir-fs': Data will be loaded from a configured directory in a file system where FHIR resources are persisted in ndjson, parquet, or delta format (currently local but hdfs, s3 will also be supported)
        #   sourceType = "fhir-api"
        sourceType = "fhir-api"
        # Base FHIR standard version that data is compliant
        version = "R5"
        # Base URL for the target FHIR server
        baseUrl = "http://127.0.0.1:8080/fhir"
        # Number of FHIR resources to request per FHIR Page (_count search parameter) while querying and loading FHIR resources from FHIR server
        recordsPerPage = 500
        # Number of FHIR resources per Spark Streaming Micro Batch. Used while querying and processing the results to limit each micro batch so that when execution is interrupted it starts from that micro batch not completely from beginning.
        recordsPerBatch = 10000
        # Number of entities to use per entity group for dividing an execution for a candidate entities into groups
        entityPerQuery = 250
        # Number of partitions for Spark for data loading. This means number  of parallel queries to target FHIR server.
        numOfPartitions = 4
        # Timeout for each FHIR request
        requestTimeout = "300 seconds"
        # Name of pagination parameter in target FHIR server
        paginationParam = "_searchafter"
        # TODO security settings for authentication/authorization
      }
    }
  }
}

# Config is required for io.onfhir.fhir-api-spark-source IF WE OVERRIDE the default Typesafe config read from the resources by giving -Dconfig.file parameter.
spark-on-fhir {
  akka {
    actor {
      provider = "local"
    }
    client {
      user-agent-header = akka-http/${akka.http.version}
      # The time period within which the TCP connecting process must be completed.
      connecting-timeout = 10s
      # The time after which an idle connection will be automatically closed.
      # Set to `infinite` to completely disable idle timeouts.
      idle-timeout = 60 s
    }
  }
}