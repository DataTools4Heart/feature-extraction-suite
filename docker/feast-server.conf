onfhir-feast {
  # Configurations for REST endpoint of onfhir-feast
  server {
    # Hostname that server will work. Using 0.0.0.0 will bind the server to both localhost and the IP of the server that you deploy it
    host = 0.0.0.0
    # Port that server will listen
    port = 8085
    # Base Uri for server e.g. With this default configuration, the root path of the Admin server will be http://localhost:8080/admin
    base-uri = onfhir-feast
    #ssl {
    # Path to the java keystore for enabling ssl for onFhir server, use null to disable ssl
    #keystore = null
    # Password of the keystore for enabling ssl for onFhir server
    #password = null
    #}
    root-url = "http://onfhir:8080/onfhir-feast"

    allowed-origins = [
      "http://127.0.0.1:4300"
    ]
  }
  # Configurations for onfhir-feast registry (persistence)
  registry {
    db = ${slick-postgres.db}
    type = "fixed"
    rootFolder ="/usr/local/onfhir-feast/conf/definitions"
  }
  # Configurations related with supported data interfaces and data sources
  etl {
    # Settings for offline-repository where the extracted population, featuregroup and datasets are stored
    offline-repository {
      # Type of file system to store
      # 'fs' : For normal file system
      # TODO support others e.g. hdfs, s3, etc
      type = "fs"
      # Absolute path to the root directory where all extracted data and temp data will be stored
      rootDir = "/usr/local/onfhir-feast/output-data"
    }

    # Data interfaces supported in onfhir-feast (key-value pair where key as the name of interface, value as the configuration)
    data-interfaces {
      # Configuration for FHIR interface
      fhir {
        implementation = "io.onfhir.feast.etl.fhir.FhirDataInterfaceProvider"
        # Supported versions within the onfhir-feast platform
        versions = ["R5"]
        # Further configuration for population extraction execution fo this interface
        population {
          # Execution mode
          # 'phased' : Execution will be divided into phases (evaluation of entry, exit, eligibility, etc) where intermediate results are stored in temp directory
          # 'single-batch': Execution is done as a single Spark Batch job
          execution-mode = "phased"
        }
        # Further configuration for FeatureGroup extraction execution fo this interface
        featureGroup {
          # Execution mode
          # 'online' : Skip synchronization and everytime load and extract entries from the data source
          # 'phased' : Synchronization step is performed in phased mode and results are stored in the dedicated feature group directory and then loaded from there when it is referenced
          execution-mode = "phased"
        }
      }
    }

    # Provides the data source settings for onfhir-feast
    data-sources {
      # Example FHIR API data source configuration (key-value pair where key as the name of data source)
      myFhirServer {
        id = "myFhirServer"
        # Data access interface (one of data-interfaces.*)
        interface = "fhir"
        # Type of data source. Only the followings are supported currently.
        #   - 'fhir-api': Data will be loaded from a target FHIR API as FHIR resources.
        #   - 'fhir-fs': Data will be loaded from a configured directory in a file system where FHIR resources are persisted in ndjson, parquet, or delta format (currently local but hdfs, s3 will also be supported)
        sourceType = "fhir-api"
        # Base FHIR standard version that data is compliant
        version = "R5"
        # Base URL for the target FHIR server
        baseUrl = "http://onfhir:8080/fhir"
        # Number of FHIR resources to request per FHIR Page (_count search parameter) while querying and loading FHIR resources from FHIR server
        recordsPerPage = 500
        # Number of FHIR resources per Spark Streaming Micro Batch. Used while querying and processing the results to limit each micro batch so that when execution is interrupted it starts from that micro batch not completely from beginning.
        recordsPerBatch = 10000
        # Number of entities to use per entity group for dividing an execution for a candidate entities into groups
        entityPerQuery = 250
        # Number of partitions for Spark for data loading. This means number  of parallel queries to target FHIR server.
        numOfPartitions = 4
        # Timeout for each FHIR request
        requestTimeout = "300 seconds"
        # Name of pagination parameter in target FHIR server
        paginationParam = "_searchafter"
        # TODO security settings for authentication/authorization
      }
    }
  }

  terminology {
    valuesets {
      registry {
        # Type of the valueset registry
        # Either fixed | registry
        # 'fs' : A fixed File system based valueset repository for ValueSet definitions from the configured folder
        # 'registry': TODO A onfhir-feast specific registry service for ValueSet definitions where they are managed dynamically
        type = "fs"
        # The root directory for all ValueSet definitions supplied for 'fixed' type terminology service
        rootDir = "/usr/local/onfhir-feast/conf/definitions/valuesets"
      }
      # TODO use relative path
      resolver {
        # Type of the ValueSet resolver
        # 'local' : Only enumerated valuesets can be resolved (For local we don't need any further configuration
        # TODO Others not implemented yet (e.g. external terminology service)
        type = "local"
      }
    }
  }


  histogram {
    # The number of bins generated for histogram generation for numeric features in the metadata
    numBins = 10
  }
}

# Configuration for Postgres database used for persistence
slick-postgres {
  profile = "slick.jdbc.PostgresProfile$"
  # Database settings
  db {
    connectionPool = "HikariCP"
    dataSourceClass = "slick.jdbc.DriverDataSource"
    serverName = "postgres"
    portNumber = 5432
    databaseName = "onfhir_feast_new"
    properties = {
      driver = "org.postgresql.Driver"
      url = "jdbc:postgresql://"${slick-postgres.db.serverName}":"${slick-postgres.db.portNumber}"/"${slick-postgres.db.databaseName}
      user = "postgres"
      password = "password"
    }
    keepAliveConnection = true
    numThreads = 5
    maxConnections = 5
    minConnections = 1
  }
}

akka {
  actor {
    provider = "cluster"
    #Serialization
    serializers {
      jackson-json = "akka.serialization.jackson.JacksonJsonSerializer"
    }
    serialization-bindings {
      "io.onfhir.feast.model.JacksonSerializable" = jackson-json
    }
  }

  remote.artery {
    canonical {
      hostname = "127.0.0.1"
      port = 2551
    }
  }

  cluster {
    seed-nodes = ["akka://onfhir-feast@127.0.0.1:2551"]
    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"
  }

  #Akka Jackson Serialiation settings (needed for cross-node message serializations for cluster setup)
  serialization.jackson {
    # Enable serialization of java.time classes
    jackson-modules += "com.fasterxml.jackson.datatype.jsr310.JavaTimeModule"
  }

  # Akka persistence settings
  persistence {
    journal.plugin = "jdbc-journal"
    auto-start-journals = ["jdbc-journal"]

    snapshot-store.plugin = "jdbc-snapshot-store"
    auto-start-snapshot-stores = ["jdbc-snapshot-store"]
  }

  #Akka projection settings
  projection {
    slick {
      # Using postgres for storing offsets
      dialect = postgres-dialect
      db = ${akka-persistence-jdbc.shared-databases.slick}
      blocking-jdbc-dispatcher.thread-pool-executor.fixed-pool-size = 3
    }
  }

  http {
    server {
      # Header for server
      server-header = onfhir-feast server
      parsing{
        uri-parsing-mode = relaxed
        max-uri-length = 128k
      }
      # Request timeout for all REST services
      request-timeout = 3000 s
      # Maximum inactivity time of a given HTTP connection
      idle-timeout = 300 s
      # Should be on in order to get IP address of the clients for audits
      remote-address-header = on
    }
  }
}

# Database Connection pool settings for Akka shared by all journals
akka-persistence-jdbc {
  shared-databases {
    slick {
      profile = ${slick-postgres.profile}
      db {
        host = ${slick-postgres.db.serverName}
        url = "jdbc:postgresql://"${slick-postgres.db.serverName}":"${slick-postgres.db.portNumber}"/"${slick-postgres.db.databaseName}"?reWriteBatchedInserts=true"
        user =  ${slick-postgres.db.properties.user}
        password = ${slick-postgres.db.properties.password}
        driver = ${slick-postgres.db.properties.driver}
        numThreads = 5
        maxConnections = 5
        minConnections = 1
      }
    }
  }
}

jdbc-journal {
  use-shared-db = "slick"
}

# the akka-persistence-snapshot-store in use
jdbc-snapshot-store {
  use-shared-db = "slick"
}

# the akka-persistence-query provider in use
jdbc-read-journal {
  use-shared-db = "slick"
}

spark-on-fhir {
  akka {
    actor {
      provider = "local"
    }
    client {
      user-agent-header = akka-http/${akka.http.version}
      # The time period within which the TCP connecting process must be completed.
      connecting-timeout = 10s
      # The time after which an idle connection will be automatically closed.
      # Set to `infinite` to completely disable idle timeouts.
      idle-timeout = 60 s
    }
  }
}


